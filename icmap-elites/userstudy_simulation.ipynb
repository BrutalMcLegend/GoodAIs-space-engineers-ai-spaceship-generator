{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire\n",
    "\n",
    "The questionnaire to ask users at the end of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale: {`Excellent`, `Good`, `Fair`, `Poor`}\n",
    "\n",
    "1. Attainment of desired/interesting solution: did you find a solution that you consider interesting or that you had in mind before the beginning of the experiment?\n",
    "2. System efficiency: was the system efficient in providing you with possible solutions?\n",
    "3. User-friendliness: how user-friendly was the system?\n",
    "4. Runtime response: how fast was the system when handling your requests?\n",
    "5. User fatigue: how tiresome was the experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_volunteers = 10\n",
    "n_questions = 5\n",
    "n_categories = 4\n",
    "\n",
    "questions_place = {\n",
    "    'A': 'Attainment of desired/interesting solution',\n",
    "    'B': 'System efficiency',\n",
    "    'C': 'User-friendliness',\n",
    "    'D': 'Runtime response',\n",
    "    'E': 'User fatigue'\n",
    "} \n",
    "\n",
    "scale = ['', 'Excellent', 'Good', 'Fair', 'Poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "responses = np.random.random((n_questions, n_categories))\n",
    "responses /= np.sum(responses, axis=1)[:, None]\n",
    "responses *= n_volunteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "axd = plt.figure(constrained_layout=True).subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AB\n",
    "    CD\n",
    "    E.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for i in range(len(list(questions_place.keys()))):\n",
    "    plot_idx = list(questions_place.keys())[i]\n",
    "    axd[plot_idx].bar(np.arange(n_categories), responses[i], 1, color='lightblue', alpha=0.75)\n",
    "    axd[plot_idx].set_xticklabels(scale)\n",
    "    axd[plot_idx].set_yticks(np.arange(0, n_volunteers + 1, 1))\n",
    "    axd[plot_idx].set_title(questions_place[plot_idx])\n",
    "    axd[plot_idx].grid()\n",
    "\n",
    "plt.suptitle('Questionnaire responses distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_response = np.mean(responses * [4,3,2,1], axis=1)\n",
    "\n",
    "plt.bar(questions_place.values(), avg_response)\n",
    "plt.ylim(0, n_volunteers)\n",
    "plt.title('Average category score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emitters ranking\n",
    "\n",
    "Simulate the ranking of emitters per type and perform the statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples_name = ['random-emitter', 'preference-matrix-emitter', 'contextual-bandit-emitter']\n",
    "samples_scores = {k:[] for k in samples_name}\n",
    "\n",
    "for _ in range(n_volunteers):\n",
    "    tmp = samples_name.copy()\n",
    "    np.random.shuffle(tmp)\n",
    "    score_v = len(samples_name)\n",
    "    for emitter_type in tmp:\n",
    "        samples_scores[emitter_type].append(score_v)# - np.ceil(len(samples_name) / 2))\n",
    "        score_v -= 1\n",
    "\n",
    "samples = [np.asarray(x) for x in samples_scores.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapiro-Wilk test\n",
    "\n",
    "Shapiro-Wilk test for normality. We want the statistic to be as close to `1` as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.tests import shapiro_wilk\n",
    "\n",
    "print('## SHAPIRO-WILK TEST ##')\n",
    "\n",
    "shapiro_test = shapiro_wilk(samples=samples)\n",
    "\n",
    "for (stat, pvalue), name in zip(shapiro_test, samples_name):\n",
    "    print(f'Result score for {name}:')\n",
    "    print(f'\\tStatistic: {stat}')\n",
    "    print(f'\\tp-value: {pvalue}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-way ANOVA\n",
    "\n",
    "The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. The test is applied to samples from two or more groups, possibly with differing sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pcgsepy.stats.tests import anova, THRESHOLD_PVALUE\n",
    "\n",
    "print('## ONE-WAY ANOVA TEST ##')\n",
    "\n",
    "anova_test = anova(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = anova_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(anova_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANOVA test has important assumptions that must be satisfied in order for the associated p-value to be valid:\n",
    "- The samples are independent.\n",
    "- Each sample is from a normally distributed population.\n",
    "- The population standard deviations of the groups are all equal. This property is known as homoscedasticity.\n",
    "\n",
    "If these assumptions are not true for a given set of data, it may still be possible to use the Kruskal-Wallis H-test (scipy.stats.kruskal) or the Alexander-Govern test (scipy.stats.alexandergovern) although with some loss of power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kruskal-Wallis H-test\n",
    "\n",
    "In case the Shapiro-Wilk test fails, we resort to the Kruskal-Wallis H-test instead of one-way ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.tests import kruskal_wallis, THRESHOLD_PVALUE\n",
    "\n",
    "print('## KRUSKAL-WALLIS TEST ##')\n",
    "\n",
    "kruskal_wallis_test = kruskal_wallis(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = kruskal_wallis_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(kruskal_wallis_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.plots import plot_rankings\n",
    "\n",
    "plot_rankings(samples=samples,\n",
    "              labels=['1st place', '2nd place', '3rd place'],\n",
    "              names=samples_name,\n",
    "              title='Emitters ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.plots import plot_scores\n",
    "\n",
    "plot_scores(samples=samples,\n",
    "            names=samples_name,\n",
    "            score_to_value={1: 1, 2: 0.5, 3: 0.25},\n",
    "            title='Emitters score'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pcg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baec60536c6749885c57d3beb549b4412d50c1c1ea218f0ac711a9872f2242c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
