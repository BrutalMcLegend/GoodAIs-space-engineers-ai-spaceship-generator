{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_csv = './Space Engineers AI Spaceship Generator Questionnaire.csv'\n",
    "file_uploads_dir = './Space Engineers AI Spaceship Generator Questionnaire (File responses)'\n",
    "metrics_dir = os.path.join(file_uploads_dir, 'metrics')\n",
    "comparator_dir = os.path.join(file_uploads_dir, 'comparator')\n",
    "\n",
    "questions_place = {\n",
    "    'A': 'Solution satisfaction',\n",
    "    'B': 'System efficiency',\n",
    "    'C': 'Runtime response',\n",
    "    'D': 'User fatigue'\n",
    "} \n",
    "\n",
    "scale = ['', 'Poor', 'Fair', 'Good', 'Excellent']\n",
    "\n",
    "samples_name = ['Human', 'Random', 'Greedy', 'Contextual Bandit']\n",
    "\n",
    "experiments = {\n",
    "    'Human': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed_emitter': [],\n",
    "        'n_interactions': [],\n",
    "        'avg_complexity': [],\n",
    "        'n_solutions_feas': [],\n",
    "        'n_solutions_infeas': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Random': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed_emitter': [],\n",
    "        'n_interactions': [],\n",
    "        'avg_complexity': [],\n",
    "        'n_solutions_feas': [],\n",
    "        'n_solutions_infeas': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Greedy': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed_emitter': [],\n",
    "        'n_interactions': [],\n",
    "        'avg_complexity': [],\n",
    "        'n_solutions_feas': [],\n",
    "        'n_solutions_infeas': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Contextual Bandit': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed_emitter': [],\n",
    "        'n_interactions': [],\n",
    "        'avg_complexity': [],\n",
    "        'n_solutions_feas': [],\n",
    "        'n_solutions_infeas': [],\n",
    "        'scores': []\n",
    "    },\n",
    "}\n",
    "\n",
    "user_friendliness = []\n",
    "feedbacks = []\n",
    "n_volunteers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_anonymizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_rng(rng_seed):\n",
    "    random.seed(rng_seed)\n",
    "    my_emitterslist = samples_name.copy()\n",
    "    random.shuffle(my_emitterslist)\n",
    "    return my_emitterslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize file uploads\n",
    "\n",
    "Files in `file_uploads_dir` are metrics and configurations ranking, but contain the name of the uploader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if needs_anonymizing:\n",
    "    for subdir, ext in zip([metrics_dir, comparator_dir], ['', '.json']):\n",
    "        files = os.listdir(subdir)\n",
    "        for f in files:\n",
    "            prefix, _ = f.split(' - ')\n",
    "            # ext = name_ext.split('.')[1]\n",
    "            os.rename(os.path.join(subdir, f),\n",
    "                      os.path.join(subdir, f'{prefix}{ext}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(comparator_dir)\n",
    "# for f in files:\n",
    "#     with open(os.path.join(comparator_dir, f), 'r') as fin:\n",
    "#         content = fin.read()\n",
    "#     with open(os.path.join(comparator_dir, f), 'w') as fout:\n",
    "#         content = content.replace('\\'', '\\\"')\n",
    "#         fout.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the `csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(questionnaire_csv, newline='', encoding='utf8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    for row in reader:\n",
    "        rng_seed = int(row['Please insert your ID here'])\n",
    "        \n",
    "        try:\n",
    "            with open(os.path.join(comparator_dir, f'{rng_seed}_res.json'), 'r') as f:\n",
    "                scores = json.loads(f.read())\n",
    "            \n",
    "            with open(os.path.join(metrics_dir, f'user_metrics_{rng_seed}')) as f:\n",
    "                metrics = json.loads(f.read())\n",
    "            \n",
    "            experiments_order = order_by_rng(rng_seed=rng_seed)\n",
    "            for i, v in enumerate(experiments_order):\n",
    "                experiments[v]['solution_satisfaction'].append(int(row[f'({i + 1}) Solution satisfaction']))\n",
    "                experiments[v]['system_efficiency'].append(int(row[f'({i + 1}) System variety']))\n",
    "                experiments[v]['runtime_response'].append(int(row[f'({i + 1}) Runtime response']))\n",
    "                experiments[v]['user_fatigue'].append(int(row[f'({i + 1}) Fatigue']))            \n",
    "                experiments[v]['scores'].append(int(scores[v]))\n",
    "                for metric in metrics.keys():\n",
    "                    ms = metrics[metric]\n",
    "                    k = samples_name.index(v)\n",
    "                    if isinstance(ms, list):\n",
    "                        m = ms[k]\n",
    "                    else:\n",
    "                        m = ms.get(str(k))\n",
    "                    avg_m = np.average(m)\n",
    "                    experiments[v][metric].append(avg_m)\n",
    "                    \n",
    "            user_friendliness.append(int(row['Ease of use']))\n",
    "            feedbacks.append(row['Please provide any additional feedback here'])\n",
    "            \n",
    "            n_volunteers += 1\n",
    "        except FileNotFoundError as e:\n",
    "            print('Skipped', rng_seed)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('### Experiments feedback ###')\n",
    "print(f'Average user-friendliness: {np.mean(user_friendliness)}')\n",
    "print('Additional feedbacks: ')\n",
    "for f in feedbacks:\n",
    "    if f:\n",
    "        print(f)\n",
    "print('###        -----        ###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_volunteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [np.asarray(experiments[v]['scores']) for v in samples_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.setup_utils import setup_matplotlib\n",
    "\n",
    "setup_matplotlib(type3_fix=False,\n",
    "                 larger_fonts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feedback_keys = ['solution_satisfaction', 'system_efficiency', 'runtime_response', 'user_fatigue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback per experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feedback = {v:{k: {i: 0 for i in range(len(scale))} for k in feedback_keys} for v in samples_name}\n",
    "\n",
    "for sample in samples_name:\n",
    "    for k in feedback_keys:\n",
    "        for v in experiments[sample][k]:\n",
    "            user_feedback[sample][k][v] += 1\n",
    "\n",
    "for sample in samples_name:    \n",
    "    axd = plt.figure(constrained_layout=True).subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AB\n",
    "        CD\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    for i, (plot_idx, metric) in enumerate(questions_place.items()):\n",
    "        axd[plot_idx].bar(np.arange(1, len(scale)), [user_feedback[sample][metric.replace(' ', '_').lower()][j] for j in range(1, len(scale))], 1, color='lightblue', alpha=0.75)\n",
    "        axd[plot_idx].set_xticklabels(scale)\n",
    "        axd[plot_idx].set_yticks(np.arange(0, n_volunteers + 1, n_volunteers // 5))\n",
    "        axd[plot_idx].set_title(questions_place[plot_idx])\n",
    "        axd[plot_idx].grid()\n",
    "\n",
    "    # plt.suptitle(f'Questionnaire responses distribution ({sample} emitter)')\n",
    "\n",
    "    plt.savefig(f'./plots/questionnaire-plots-{sample.replace(\" \", \"_\").lower()}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_response = {v:{k: np.mean(experiments[v][k]) for k in feedback_keys} for v in samples_name}\n",
    "\n",
    "for sample in samples_name:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(questions_place.values(), [avg_response[sample][k] for k in feedback_keys])\n",
    "    plt.ylim(0, len(feedback_keys))\n",
    "    # plt.xticks(rotation = 45)\n",
    "    # plt.title(f'Average category score ({sample} emitter)')\n",
    "    plt.savefig(f'./plots/avg-category-score-{sample.replace(\" \", \"_\").lower()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.plots import plot_rankings\n",
    "from pcgsepy.stats.plots import plot_scores\n",
    "\n",
    "display_names = [x.replace('Contextual Bandit', 'Cont. Param.') for x in samples_name]\n",
    "\n",
    "plot_rankings(samples=samples,\n",
    "              labels=['1st place', '2nd place', '3rd place', '4th place'],\n",
    "              names=display_names,\n",
    "              title='',\n",
    "              filename='./plots/emitters-rankings')\n",
    "\n",
    "plot_scores(samples=samples,\n",
    "            names=display_names,\n",
    "            score_to_value={1: 4, 2: 3, 3: 2, 4: 1},\n",
    "            title='',\n",
    "            filename='./plots/emitters-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, sample in zip(samples_name, samples):\n",
    "    print(name)\n",
    "    print(f'#1: {np.sum([1 if x == 1 else 0 for x in sample])}')\n",
    "    print(f'#2: {np.sum([1 if x == 2 else 0 for x in sample])}')\n",
    "    print(f'#3: {np.sum([1 if x == 3 else 0 for x in sample])}')\n",
    "    print(f'#4: {np.sum([1 if x == 4 else 0 for x in sample])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_elapsed_emitter = [np.mean(experiments[v]['time_elapsed_emitter']) for v in samples_name]\n",
    "\n",
    "\n",
    "plt.bar(display_names, avg_time_elapsed_emitter)\n",
    "plt.ylim(0, max(avg_time_elapsed_emitter))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg-time-elapsed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_n_interactions = [np.mean(experiments[v]['n_interactions']) for v in samples_name]\n",
    "\n",
    "plt.bar(display_names, avg_n_interactions)\n",
    "plt.ylim(0, max(avg_n_interactions))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg_n_interactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_complexity = [np.mean(experiments[v]['avg_complexity']) for v in samples_name]\n",
    "\n",
    "plt.bar(display_names, avg_complexity)\n",
    "plt.ylim(0, max(avg_complexity))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg_complexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_solutions_feas = [np.mean(experiments[v]['n_solutions_feas']) for v in samples_name]\n",
    "n_solutions_infeas = [np.mean(experiments[v]['n_solutions_infeas']) for v in samples_name]\n",
    "\n",
    "n_solutions = [nf + ninf for nf, ninf in zip(n_solutions_feas, n_solutions_infeas)]\n",
    "\n",
    "plt.bar(display_names, n_solutions)\n",
    "plt.ylim(0, max(n_solutions))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/n_solutions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_header = \"\"\"\n",
    "\\\\begin{table}[!t]\n",
    "        \\\\centering\n",
    "        \\\\resizebox{.5\\\\textwidth}{!}{%\n",
    "        \\\\begin{tabular}{|l|l|cccc|c|}\n",
    "        \\\\hline\n",
    "        \\\\multicolumn{1}{|c|}{\\\\multirow{2}{*}{\\\\textbf{Emitter}}} & \\\\multirow{2}{*}{Metric} & \\\\multicolumn{4}{c|}{Score} & \\\\multirow{2}{*}{Rank} \\\\\\\\\n",
    "        \\\\multicolumn{1}{|c|}{} &  & Poor & Fair & Good & Excellent &  \\\\\\\\ \\\\hline\n",
    "\"\"\"\n",
    "\n",
    "table_body = []\n",
    "\n",
    "table_body_module = \"\"\"\n",
    "        \\\\multirow{4}{*}{EMITTERNAME} & Solution Satisfaction & SSPOOR & SSFAIR & SSGOOD & SSEXCELLENT & SSRANK \\\\\\\\\n",
    "         & System Efficiency & SEPOOR & SEFAIR & SEGOOD & SEEXCELLENT & SERANK \\\\\\\\\n",
    "         & Runtime Response & RRPOOR & RRFAIR & RRGOOD & RREXCELLENT & RRRANK \\\\\\\\\n",
    "         & User fatigue & UFPOOR & UFFAIR & UFGOOD & UFEXCELLENT & UFRANK \\\\\\\\ \\\\hline\n",
    "\"\"\"\n",
    "\n",
    "table_footer = \"\"\"\n",
    "        \\\\end{tabular}%\n",
    "        }\n",
    "        \\\\caption{Questionnaire results obtained with N candidates using the different emitters.}\n",
    "        \\\\label{tab:questionnaire-res}\n",
    "    \\\\end{table}\n",
    "\"\"\".replace('N', str(n_volunteers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_scores = {}\n",
    "feedback_ranks = {}\n",
    "\n",
    "for sample in samples_name:\n",
    "    feedback_scores[sample] = {}\n",
    "    feedback_ranks[sample] = {}\n",
    "    for k in feedback_keys:\n",
    "        d = user_feedback[sample][k]\n",
    "        score = np.sum(np.multiply(list(d.keys()), list(d.values())))\n",
    "        feedback_scores[sample][k] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in feedback_keys:\n",
    "    emitters = sorted(samples_name, key=lambda x: feedback_scores[x][k], reverse=True)\n",
    "    for sample in samples_name:\n",
    "        feedback_ranks[sample][k] = emitters.index(sample) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples_name:\n",
    "    emitter_table = table_body_module.replace('EMITTERNAME', sample)\n",
    "    \n",
    "    emitter_table = emitter_table.replace('SSPOOR', str(user_feedback[sample]['solution_satisfaction'][1]))\n",
    "    emitter_table = emitter_table.replace('SSFAIR', str(user_feedback[sample]['solution_satisfaction'][2]))\n",
    "    emitter_table = emitter_table.replace('SSGOOD', str(user_feedback[sample]['solution_satisfaction'][3]))\n",
    "    emitter_table = emitter_table.replace('SSEXCELLENT', str(user_feedback[sample]['solution_satisfaction'][4]))\n",
    "    emitter_table = emitter_table.replace('SSRANK', str(feedback_ranks[sample]['solution_satisfaction']))\n",
    "    \n",
    "    emitter_table = emitter_table.replace('SEPOOR', str(user_feedback[sample]['system_efficiency'][1]))\n",
    "    emitter_table = emitter_table.replace('SEFAIR', str(user_feedback[sample]['system_efficiency'][2]))\n",
    "    emitter_table = emitter_table.replace('SEGOOD', str(user_feedback[sample]['system_efficiency'][3]))\n",
    "    emitter_table = emitter_table.replace('SEEXCELLENT', str(user_feedback[sample]['system_efficiency'][4]))\n",
    "    emitter_table = emitter_table.replace('SERANK', str(feedback_ranks[sample]['system_efficiency']))\n",
    "    \n",
    "    emitter_table = emitter_table.replace('RRPOOR', str(user_feedback[sample]['runtime_response'][1]))\n",
    "    emitter_table = emitter_table.replace('RRFAIR', str(user_feedback[sample]['runtime_response'][2]))\n",
    "    emitter_table = emitter_table.replace('RRGOOD', str(user_feedback[sample]['runtime_response'][3]))\n",
    "    emitter_table = emitter_table.replace('RREXCELLENT', str(user_feedback[sample]['runtime_response'][4]))\n",
    "    emitter_table = emitter_table.replace('RRRANK', str(feedback_ranks[sample]['runtime_response']))\n",
    "    \n",
    "    emitter_table = emitter_table.replace('UFPOOR', str(user_feedback[sample]['user_fatigue'][1]))\n",
    "    emitter_table = emitter_table.replace('UFFAIR', str(user_feedback[sample]['user_fatigue'][2]))\n",
    "    emitter_table = emitter_table.replace('UFGOOD', str(user_feedback[sample]['user_fatigue'][3]))\n",
    "    emitter_table = emitter_table.replace('UFEXCELLENT', str(user_feedback[sample]['user_fatigue'][4]))\n",
    "    emitter_table = emitter_table.replace('UFRANK', str(feedback_ranks[sample]['user_fatigue']))\n",
    "    \n",
    "    table_body.append(emitter_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_body = '\\n'.join(table_body)\n",
    "print(f'{table_header}\\n{table_body}\\n{table_footer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.tests import shapiro_wilk\n",
    "\n",
    "print('## SHAPIRO-WILK TEST ##')\n",
    "\n",
    "shapiro_test = shapiro_wilk(samples=samples)\n",
    "\n",
    "for (stat, pvalue), name in zip(shapiro_test, samples_name):\n",
    "    print(f'Result score for {name}:')\n",
    "    print(f'\\tStatistic: {stat}')\n",
    "    print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pcgsepy.stats.tests import anova, THRESHOLD_PVALUE\n",
    "\n",
    "print('## ONE-WAY ANOVA TEST ##')\n",
    "\n",
    "anova_test = anova(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = anova_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(anova_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kruskal-Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pcgsepy.stats.tests import kruskal_wallis\n",
    "\n",
    "print('## KRUSKAL-WALLIS TEST ##')\n",
    "\n",
    "kruskal_wallis_test = kruskal_wallis(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = kruskal_wallis_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(kruskal_wallis_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "first_places = []\n",
    "for sample in samples:\n",
    "    onehot_sample = []\n",
    "    for elem in sample:\n",
    "        if elem == 4:\n",
    "            onehot_sample.append(1)\n",
    "        else:\n",
    "            onehot_sample.append(0)\n",
    "    first_places.append(onehot_sample)\n",
    "\n",
    "ttest_ind(first_places[1], first_places[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pcgsepy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f8655f881cc8e1843151fba8c3cd5c3e7360a4f2a04b3445baee4178248b8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
