{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_csv = './Space Engineers AI Spaceship Generator Questionnaire.csv'\n",
    "file_uploads_dir = './Space Engineers AI Spaceship Generator Questionnaire (File responses)'\n",
    "\n",
    "questions_place = {\n",
    "    'A': 'Solution satisfaction',\n",
    "    'B': 'System efficiency',\n",
    "    'C': 'Runtime response',\n",
    "    'D': 'User fatigue'\n",
    "} \n",
    "\n",
    "scale = ['', 'Poor', 'Fair', 'Good', 'Excellent']\n",
    "\n",
    "samples_name = ['Human', 'Random', 'Greedy', 'Contextual Bandit']\n",
    "\n",
    "experiments = {\n",
    "    'Human': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed': [],\n",
    "        'n_interactions': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Random': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed': [],\n",
    "        'n_interactions': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Greedy': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed': [],\n",
    "        'n_interactions': [],\n",
    "        'scores': []\n",
    "    },\n",
    "    'Contextual Bandit': {\n",
    "        'solution_satisfaction': [],\n",
    "        'system_efficiency': [],\n",
    "        'runtime_response': [],\n",
    "        'user_fatigue': [],\n",
    "        'time_elapsed': [],\n",
    "        'n_interactions': [],\n",
    "        'scores': []\n",
    "    },\n",
    "}\n",
    "\n",
    "user_friendliness = []\n",
    "feedbacks = []\n",
    "n_volunteers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_anonymizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def order_by_rng(rng_seed):\n",
    "    random.seed(rng_seed)\n",
    "    my_emitterslist = samples_name.copy()\n",
    "    random.shuffle(my_emitterslist)\n",
    "    return my_emitterslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymize file uploads\n",
    "\n",
    "Files in `file_uploads_dir` are metrics and configurations ranking, but contain the name of the uploader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if needs_anonymizing:\n",
    "    files = os.listdir(file_uploads_dir)\n",
    "    for f in files:\n",
    "        prefix, name_ext = f.split(' - ')\n",
    "        ext = name_ext.split('.')[1]\n",
    "        os.rename(os.path.join(file_uploads_dir, f),\n",
    "                os.path.join(file_uploads_dir, f'{prefix}.{ext}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the `csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "with open(questionnaire_csv, newline='', encoding='utf8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    for row in reader:\n",
    "        rng_seed = int(row['Please insert your ID here'])\n",
    "        experiments_order = order_by_rng(rng_seed=rng_seed)\n",
    "        for i, v in enumerate(experiments_order):\n",
    "            experiments[v]['solution_satisfaction'].append(int(row[f'({i + 1}) Solution satisfaction']))\n",
    "            experiments[v]['system_efficiency'].append(int(row[f'({i + 1}) System variety']))\n",
    "            experiments[v]['runtime_response'].append(int(row[f'({i + 1}) Runtime response']))\n",
    "            experiments[v]['user_fatigue'].append(int(row[f'({i + 1}) Fatigue']))\n",
    "        \n",
    "        with open(os.path.join(file_uploads_dir, f'{rng_seed}_res.json'), 'r') as f:\n",
    "            scores = json.loads(f.read())\n",
    "            for v in experiments_order:\n",
    "                experiments[v]['scores'].append(int(scores[v]))\n",
    "        \n",
    "        with open(os.path.join(file_uploads_dir, f'user_metrics_{rng_seed}.txt')) as f:\n",
    "            metrics = json.loads(f.read())\n",
    "            for metric in metrics.keys():\n",
    "                for v in experiments_order:\n",
    "                    experiments[v][metric].append(metrics[metric][samples_name.index(v)])\n",
    "        \n",
    "        user_friendliness.append(int(row['Ease of use']))\n",
    "        feedbacks.append(row['Please provide any additional feedback here'])\n",
    "        \n",
    "        n_volunteers += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('### Experiments feedback ###')\n",
    "print(f'Average user-friendliness: {np.mean(user_friendliness)}')\n",
    "print('Additional feedbacks: ')\n",
    "for f in feedbacks:\n",
    "    if f:\n",
    "        print(f)\n",
    "print('###        -----        ###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [np.asarray(experiments[v]['scores']) for v in samples_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feedback_keys = ['solution_satisfaction', 'system_efficiency', 'runtime_response', 'user_fatigue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback per experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feedback = {v:{k: {i: 0 for i in range(len(scale))} for k in feedback_keys} for v in samples_name}\n",
    "\n",
    "for sample in samples_name:\n",
    "    for k in feedback_keys:\n",
    "        for v in experiments[sample][k]:\n",
    "            user_feedback[sample][k][v] += 1\n",
    "\n",
    "for sample in samples_name:    \n",
    "    axd = plt.figure(constrained_layout=True).subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AB\n",
    "        CD\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    for i, (plot_idx, metric) in enumerate(questions_place.items()):\n",
    "        axd[plot_idx].bar(np.arange(len(scale)), [user_feedback[sample][metric.replace(' ', '_').lower()][j] for j in range(len(scale))], 1, color='lightblue', alpha=0.75)\n",
    "        axd[plot_idx].set_xticklabels(scale)\n",
    "        axd[plot_idx].set_yticks(np.arange(0, n_volunteers + 1, 1))\n",
    "        axd[plot_idx].set_title(questions_place[plot_idx])\n",
    "        axd[plot_idx].grid()\n",
    "\n",
    "    # plt.suptitle(f'Questionnaire responses distribution ({sample} emitter)')\n",
    "\n",
    "    plt.savefig(f'./plots/questionnaire-plots-{sample.replace(\" \", \"_\").lower()}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_response = {v:{k: np.mean(experiments[v][k]) for k in feedback_keys} for v in samples_name}\n",
    "\n",
    "for sample in samples_name:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(questions_place.values(), [avg_response[sample][k] for k in feedback_keys])\n",
    "    plt.ylim(0, len(feedback_keys))\n",
    "    # plt.xticks(rotation = 45)\n",
    "    # plt.title(f'Average category score ({sample} emitter)')\n",
    "    plt.savefig(f'./plots/avg-category-score-{sample.replace(\" \", \"_\").lower()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.plots import plot_rankings\n",
    "from pcgsepy.stats.plots import plot_scores\n",
    "\n",
    "display_names = [x.replace('Bandit', 'Parametric') for x in samples_name]\n",
    "\n",
    "plot_rankings(samples=samples,\n",
    "              labels=['4th place', '3rd place', '2nd place', '1st place'],\n",
    "              names=display_names,\n",
    "              title='',\n",
    "              filename='./plots/emitters-rankings')\n",
    "\n",
    "plot_scores(samples=samples,\n",
    "            names=display_names,\n",
    "            score_to_value={1: 1, 2: 0.5, 3: 0.25, 4: 0.125},\n",
    "            title='',\n",
    "            filename='./plots/emitters-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_elapsed_user = [np.mean(experiments[v]['time_elapsed_user']) for v in samples_name]\n",
    "avg_time_elapsed_emitter = [np.mean(experiments[v]['time_elapsed_emitter']) for v in samples_name]\n",
    "\n",
    "avg_time_elapsed = avg_time_elapsed_user + avg_time_elapsed_emitter\n",
    "\n",
    "plt.bar(samples_name, avg_time_elapsed)\n",
    "plt.ylim(0, max(avg_time_elapsed))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg-time-elapsed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_n_interactions = [np.mean(experiments[v]['n_interactions']) for v in samples_name]\n",
    "\n",
    "plt.bar(samples_name, avg_n_interactions)\n",
    "plt.ylim(0, max(avg_n_interactions))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg_n_interactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_complexity = [np.mean(experiments[v]['avg_complexity']) for v in samples_name]\n",
    "\n",
    "plt.bar(samples_name, avg_complexity)\n",
    "plt.ylim(0, max(avg_complexity))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/avg_complexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_solutions_feas = [np.mean(experiments[v]['n_solutions_feas']) for v in samples_name]\n",
    "n_solutions_infeas = [np.mean(experiments[v]['n_solutions_infeas']) for v in samples_name]\n",
    "\n",
    "n_solutions = n_solutions_feas + n_solutions_infeas\n",
    "\n",
    "plt.bar(samples_name, n_solutions)\n",
    "plt.ylim(0, max(n_solutions))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.savefig('./plots/n_solutions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"\"\"\n",
    "\\\\begin{table}[!t]\n",
    "    \\\\centering\n",
    "    \\\\resizebox{.4\\\\textwidth}{!}{%\n",
    "        \\\\begin{tabular}{c|c|c|c|c|}\n",
    "            \\\\cline{2-5}\n",
    "            \\\\multicolumn{1}{l|}{}                                            & Poor  & Fair & Good & Excellent \\\\\\\\ \\hline\n",
    "            \\\\multicolumn{1}{|c|}{Solution satisfaction}                      & SSPOOR    & SSFAIR    & SSGOOD    & SSEXCELLENT         \\\\\\\\ \\hline\n",
    "            \\\\multicolumn{1}{|c|}{System efficiency}                          & SEPOOR    & SEFAIR    & SEGOOD    & SEEXCELLENT         \\\\\\\\ \\hline\n",
    "            \\\\multicolumn{1}{|c|}{Runtime response}                           & RRPOOR    & RRFAIR    & RRGOOD    & RREXCELLENT         \\\\\\\\ \\hline\n",
    "            \\\\multicolumn{1}{|c|}{User fatigue}                               & UFPOOR    & UFFAIR    & UFGOOD    & UFEXCELLENT         \\\\\\\\ \\hline\n",
    "        \\\\end{tabular}\n",
    "    }\n",
    "    \\\\caption{Questionnaire results obtained with N candidates using the EMITTERNAME2 emitter.}\n",
    "    \\\\label{tab:questionnaire-EMITTERNAME1}\n",
    "\\end{table}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples_name:\n",
    "    emitter_table = table.replace('EMITTERNAME1', sample.replace(' ', '_').lower())\n",
    "    emitter_table = emitter_table.replace('EMITTERNAME2', sample)\n",
    "    \n",
    "    emitter_table = emitter_table.replace('SSPOOR', str(user_feedback[sample]['solution_satisfaction'][1]))\n",
    "    emitter_table = emitter_table.replace('SSFAIR', str(user_feedback[sample]['solution_satisfaction'][2]))\n",
    "    emitter_table = emitter_table.replace('SSGOOD', str(user_feedback[sample]['solution_satisfaction'][3]))\n",
    "    emitter_table = emitter_table.replace('SSEXCELLENT', str(user_feedback[sample]['solution_satisfaction'][4]))\n",
    "    \n",
    "    \n",
    "    emitter_table = emitter_table.replace('SEPOOR', str(user_feedback[sample]['system_efficiency'][1]))\n",
    "    emitter_table = emitter_table.replace('SEFAIR', str(user_feedback[sample]['system_efficiency'][2]))\n",
    "    emitter_table = emitter_table.replace('SEGOOD', str(user_feedback[sample]['system_efficiency'][3]))\n",
    "    emitter_table = emitter_table.replace('SEEXCELLENT', str(user_feedback[sample]['system_efficiency'][4]))\n",
    "    \n",
    "    \n",
    "    emitter_table = emitter_table.replace('RRPOOR', str(user_feedback[sample]['runtime_response'][1]))\n",
    "    emitter_table = emitter_table.replace('RRFAIR', str(user_feedback[sample]['runtime_response'][2]))\n",
    "    emitter_table = emitter_table.replace('RRGOOD', str(user_feedback[sample]['runtime_response'][3]))\n",
    "    emitter_table = emitter_table.replace('RREXCELLENT', str(user_feedback[sample]['runtime_response'][4]))\n",
    "    \n",
    "    \n",
    "    emitter_table = emitter_table.replace('UFPOOR', str(user_feedback[sample]['user_fatigue'][1]))\n",
    "    emitter_table = emitter_table.replace('UFFAIR', str(user_feedback[sample]['user_fatigue'][2]))\n",
    "    emitter_table = emitter_table.replace('UFGOOD', str(user_feedback[sample]['user_fatigue'][3]))\n",
    "    emitter_table = emitter_table.replace('UFEXCELLENT', str(user_feedback[sample]['user_fatigue'][4]))\n",
    "    \n",
    "    emitter_table = emitter_table.replace('N', str(n_volunteers))\n",
    "    \n",
    "    print(emitter_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgsepy.stats.tests import shapiro_wilk\n",
    "\n",
    "print('## SHAPIRO-WILK TEST ##')\n",
    "\n",
    "shapiro_test = shapiro_wilk(samples=samples)\n",
    "\n",
    "for (stat, pvalue), name in zip(shapiro_test, samples_name):\n",
    "    print(f'Result score for {name}:')\n",
    "    print(f'\\tStatistic: {stat}')\n",
    "    print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pcgsepy.stats.tests import anova, THRESHOLD_PVALUE\n",
    "\n",
    "print('## ONE-WAY ANOVA TEST ##')\n",
    "\n",
    "anova_test = anova(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = anova_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(anova_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kruskal-Wallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pcgsepy.stats.tests import kruskal_wallis\n",
    "\n",
    "print('## KRUSKAL-WALLIS TEST ##')\n",
    "\n",
    "kruskal_wallis_test = kruskal_wallis(samples=samples)\n",
    "\n",
    "overall_stat, overall_pvalue = kruskal_wallis_test[0]\n",
    "\n",
    "print('Overall score:')\n",
    "print(f'\\tStatistic: {overall_stat}')\n",
    "print(f'\\tp-value: {overall_pvalue}')\n",
    "\n",
    "if overall_pvalue < THRESHOLD_PVALUE:\n",
    "    for (stat, pvalue), name in zip(kruskal_wallis_test[1:], itertools.combinations(iterable=samples_name, r=2)):\n",
    "        print(f'Result score for {\" x \".join(list(name))}:')\n",
    "        print(f'\\tStatistic: {stat}')\n",
    "        print(f'\\tp-value: {pvalue}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pcg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baec60536c6749885c57d3beb549b4412d50c1c1ea218f0ac711a9872f2242c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
